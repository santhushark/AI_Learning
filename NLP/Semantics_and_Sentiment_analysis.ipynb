{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53021e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 120.9MB 29.7MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "  Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-md-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/en_core_web_md\n",
      "    -->\n",
      "    /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/spacy/data/en_core_web_md\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_md')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdc2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 34.1MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-lg-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/en_core_web_lg\n",
      "    -->\n",
      "    /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7c7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ce3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156950f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8963e-01, -4.0309e-01,  3.5350e-01, -4.7907e-01, -4.3311e-01,\n",
       "        2.3857e-01,  2.6962e-01,  6.4332e-02,  3.0767e-01,  1.3712e+00,\n",
       "       -3.7582e-01, -2.2713e-01, -3.5657e-01, -2.5355e-01,  1.7543e-02,\n",
       "        3.3962e-01,  7.4723e-02,  5.1226e-01, -3.9759e-01,  5.1333e-03,\n",
       "       -3.0929e-01,  4.8911e-02, -1.8610e-01, -4.1702e-01, -8.1639e-01,\n",
       "       -1.6908e-01, -2.6246e-01, -1.5983e-02,  1.2479e-01, -3.7276e-02,\n",
       "       -5.7125e-01, -1.6296e-01,  1.2376e-01, -5.5464e-02,  1.3244e-01,\n",
       "        2.7519e-02,  1.2592e-01, -3.2722e-01, -4.9165e-01, -3.5559e-01,\n",
       "       -3.0630e-01,  6.1185e-02, -1.6932e-01, -6.2405e-02,  6.5763e-01,\n",
       "       -2.7925e-01, -3.0450e-03, -2.2400e-02, -2.8015e-01, -2.1975e-01,\n",
       "       -4.3188e-01,  3.9864e-02, -2.2102e-01, -4.2693e-02,  5.2748e-02,\n",
       "        2.8726e-01,  1.2315e-01, -2.8662e-02,  7.8294e-02,  4.6754e-01,\n",
       "       -2.4589e-01, -1.1064e-01,  7.2250e-02, -9.4980e-02, -2.7548e-01,\n",
       "       -5.4097e-01,  1.2823e-01, -8.2408e-02,  3.1035e-01, -6.3394e-02,\n",
       "       -7.3755e-01, -5.4992e-01,  9.9999e-02, -2.0758e-01, -3.9674e-02,\n",
       "        2.0664e-01, -9.7557e-02, -3.7092e-01,  2.7901e-01, -6.2218e-01,\n",
       "       -1.0280e-01,  2.3271e-01,  4.3838e-01,  3.2445e-02, -2.9866e-01,\n",
       "       -7.3611e-02,  7.1594e-01,  1.4241e-01,  2.7770e-01, -3.9892e-01,\n",
       "        3.6656e-02,  1.5759e-01,  8.2014e-02, -5.7343e-01,  3.5457e-01,\n",
       "        2.2491e-01, -6.2699e-01, -8.8106e-02,  2.4361e-01,  3.8533e-01,\n",
       "       -1.4083e-01,  1.7691e-01,  7.0897e-02,  1.7951e-01, -4.5907e-01,\n",
       "       -8.2120e-01, -2.6631e-02,  6.2549e-02,  4.2415e-01, -8.9630e-02,\n",
       "       -2.4654e-01,  1.4156e-01,  4.0187e-01, -4.1232e-01,  8.4516e-02,\n",
       "       -1.0626e-01,  7.3145e-01,  1.9217e-01,  1.4240e-01,  2.8511e-01,\n",
       "       -2.9454e-01, -2.1948e-01,  9.0460e-01, -1.9098e-01, -1.0340e+00,\n",
       "       -1.5754e-01, -1.1964e-01,  4.9888e-01, -1.0624e+00, -3.2820e-01,\n",
       "       -1.1232e-02, -7.9482e-01,  3.7275e-01, -6.8710e-03, -2.5772e-01,\n",
       "       -4.7005e-01, -4.1387e-01, -6.4089e-02, -2.8033e-01, -4.0778e-02,\n",
       "       -2.4866e+00,  6.2494e-03, -1.0210e-02,  1.2752e-01,  3.4965e-01,\n",
       "       -1.2571e-01,  3.1570e-01,  4.1926e-01,  2.0056e-01, -5.5984e-01,\n",
       "       -2.2801e-01,  1.2012e-01, -2.0518e-03, -8.9764e-02, -8.0373e-02,\n",
       "        1.1969e-02, -2.6978e-01,  3.4829e-01,  7.3664e-03, -1.1137e-01,\n",
       "        6.3410e-01,  3.8449e-01, -6.2248e-01,  4.1145e-02,  2.5922e-01,\n",
       "        6.5811e-01, -4.9548e-01, -1.3030e-01, -3.8279e-01,  1.1156e-01,\n",
       "       -4.3085e-01,  3.4473e-01,  2.7109e-02, -2.5108e-01, -2.8011e-01,\n",
       "        2.1662e-01,  3.2660e-01,  5.5895e-02,  7.6077e-02, -5.2480e-02,\n",
       "        4.5928e-02, -2.5266e-01,  5.2845e-01, -1.3145e-01, -1.2453e-01,\n",
       "        4.0556e-01,  3.1877e-01,  2.4415e-02, -2.2620e-01, -6.1960e-01,\n",
       "       -4.0886e-01, -3.5534e-02, -5.5123e-03,  2.3438e-01,  8.7854e-01,\n",
       "       -2.5161e-01,  4.0600e-01, -4.4284e-01,  3.4934e-01, -5.6429e-01,\n",
       "       -2.3676e-01,  6.2199e-01, -2.8175e-01,  4.2024e-01,  1.0043e-01,\n",
       "       -1.4720e-01,  4.9593e-01, -3.5850e-01, -1.3998e-01, -2.7494e-01,\n",
       "        2.3827e-01,  5.7268e-01,  7.9025e-02,  1.7872e-02, -2.1829e-01,\n",
       "        5.5050e-02, -5.4200e-01,  1.6788e-01,  3.9065e-01,  3.0209e-01,\n",
       "        2.3040e-01, -3.9351e-02, -2.1078e-01, -2.7224e-01,  1.6907e-01,\n",
       "        5.4819e-01,  9.4888e-02,  7.9798e-01, -6.6158e-02,  1.9844e-01,\n",
       "        2.0307e-01,  4.4808e-02, -1.0240e-01, -6.9909e-02, -3.6756e-02,\n",
       "        9.5159e-02, -2.7830e-01, -1.0597e-01, -1.6276e-01, -1.8211e-01,\n",
       "       -3.1897e-01, -2.1633e-01,  1.4994e-01, -7.2057e-02,  2.2264e-01,\n",
       "       -4.5551e-01,  3.0341e-01,  1.8431e-01,  2.1681e-01, -3.1940e-01,\n",
       "        2.6426e-01,  5.8106e-01,  5.4635e-02,  6.3238e-01,  4.3169e-01,\n",
       "        9.0343e-02,  1.9494e-01,  3.5483e-01, -2.0706e-02, -7.3117e-01,\n",
       "        1.2941e-01,  1.7418e-01, -1.5065e-01,  5.3355e-02,  4.4794e-02,\n",
       "       -1.6600e-01,  2.2007e-01, -5.3970e-01, -2.4968e-01, -2.6464e-01,\n",
       "       -5.5515e-01,  5.8242e-01,  2.2295e-01,  2.4433e-01,  4.5275e-01,\n",
       "        3.4693e-01,  1.2255e-01, -3.9059e-02, -3.2749e-01, -2.7891e-01,\n",
       "        1.3766e-01,  3.8392e-01,  1.0543e-03, -1.0242e-02,  4.9205e-01,\n",
       "       -1.7922e-01,  4.1215e-02,  1.3547e-01, -2.0598e-01, -2.3194e-01,\n",
       "       -7.7701e-01, -3.8237e-01, -7.6383e-01,  1.9418e-01, -1.5441e-01,\n",
       "        8.9740e-01,  3.0626e-01,  4.0376e-01,  2.1738e-01, -3.8050e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'lion').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959b7cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'The quick brown fox jumped').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1075b",
   "metadata": {},
   "source": [
    "Above the vector size is still 300 because it takes the average of the vectors of all the words inside the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab13cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'lion cat pet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e3883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.5265437\n",
      "lion pet 0.39923772\n",
      "cat lion 0.5265437\n",
      "cat cat 1.0\n",
      "cat pet 0.7505456\n",
      "pet lion 0.39923772\n",
      "pet cat 0.7505456\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012fca2",
   "metadata": {},
   "source": [
    "The similarity between tokens is expressed from 0-1. Since we have a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2ec161",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'like love hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5e2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.65790397\n",
      "like hate 0.6574652\n",
      "love like 0.65790397\n",
      "love love 1.0\n",
      "love hate 0.6393099\n",
      "hate like 0.6574652\n",
      "hate love 0.6393099\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e266dd",
   "metadata": {},
   "source": [
    "Here if we see \"Love\" and \"hate\" are exactly opposite but still have a similarity score close to that of \"like\" and \"love\". This is because \"Love\" and \"Hate\" are sometimes used in similar context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fd3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"dog cat nargil santosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02c3aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "nargil False 0.0 True\n",
      "santosh True 6.9398174 True\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41739d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda vec1,vec2: 1-spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8fdb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f41266e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector = king -man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65733fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    if word.has_vector and word.is_lower and word.is_alpha:\n",
    "        similarity  = cosine_similarity(new_vector, word.vector)\n",
    "        computed_similarities.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e9cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = sorted(computed_similarities, key=lambda item:-item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c2a4b",
   "metadata": {},
   "source": [
    "Here \"-\" symbol indicates sorting in descending order and index 1 indicates sorting on the similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f194dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'queen', 'prince', 'kings', 'princess', 'royal', 'throne', 'queens', 'monarch', 'kingdom']\n"
     ]
    }
   ],
   "source": [
    "print([t[0].text for t in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb251e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = computed_similarities[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7908c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<spacy.lexeme.Lexeme object at 0x7f7c59d0d5a0>, 0.018078623339533806)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c59f9f0a0>, 0.034739524126052856)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c5a49a0f0>, -0.06031614914536476)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c5b4fa370>, -0.0755949541926384)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c5a3a42d0>, 0.06920962780714035)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84f41918",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = sorted(x, key=lambda item:-item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2969180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<spacy.lexeme.Lexeme object at 0x7f7c5a3a42d0>, 0.06920962780714035)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c59f9f0a0>, 0.034739524126052856)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c59d0d5a0>, 0.018078623339533806)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c5a49a0f0>, -0.06031614914536476)\n",
      "(<spacy.lexeme.Lexeme object at 0x7f7c5b4fa370>, -0.0755949541926384)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(x_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca0fb0",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b13ffe",
   "metadata": {},
   "source": [
    "We have already explored text classification and using it to predict sentiment labels on pre-labeled movie reviews. \n",
    "But what if we don't already have those labels?\n",
    "\n",
    "    VADER(Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive / negative) and intensity (strength) of emotion.\n",
    "    \n",
    "    It is available in NLTK package and can be applied directly to unlabeled text data. \n",
    "    \n",
    "    Primarily VADER sentiment analysis relies on a dictionary which maps lexical features to emotion intensities called sentiment scores. \n",
    "    The sentiment score of text can be obtained by summing up the intensity of each word in the text\n",
    "    \n",
    "    For example words like \"love, like, enjoy, happy\" all convey a positive sentiment. VADER is intelligent to understand the basic context of these words, such as \"did not love\" as a negative sentiment.\n",
    "    \n",
    "    It also understands capitalization and punctuation, such as \"LOVE!!!\". THE word \"love\" in small letters has less weightage than the word \"love\" in all capitals and exclamation mark. \n",
    "    \n",
    "    Sentiment analysis on raw text is always challenging however, due to a variety of possible factors:\n",
    "    => Positive and Negative sentiment in the same text data. Like in a movie review they praise the actor for his acting but will write negative remarks on the screenplay\n",
    "    => Sarcasm using positive words in a negative way.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5c490f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e1dfbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/santosh/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7666c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a0bdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98a84af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"This is a good movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7092539c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a6e4d",
   "metadata": {},
   "source": [
    "The above returns a dictionary of 4 values \"neg, neu, pos, compound\", \"compound\" is basically a value obtained after normalizing the other 3 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e987f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"This was the best, most awesome movie EVER MADE!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf4c22c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b0b26",
   "metadata": {},
   "source": [
    "Here if we see because of the exclamation mark, the positive scores improves and the neutral score drops and the overall compound scores improves very rapidly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d43e1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"This was the WORST movie that has ever disgraced the screen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bddbe67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.465, 'neu': 0.535, 'pos': 0.0, 'compound': -0.8331}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479df510",
   "metadata": {},
   "source": [
    "Here if we see the compound score went to negative. If,\n",
    "\n",
    "compound > 0.0 (A somewhat positive sentiment)\n",
    "compound == 0.0 (neutral sentiment)\n",
    "compound < 0.0 (A somewhat negative sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbed38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
