{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4e7777",
   "metadata": {},
   "source": [
    "SPACY is a open source natural language processing library in python\n",
    "\n",
    "NLTK is natural language toolkit. It is one more open source natural language processing library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e252ef3",
   "metadata": {},
   "source": [
    "#######\n",
    "SPACY BASICS\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbf7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37caae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here this line of code is called loading the model\n",
    "# 'en_core_web_sm' is a smaller version of the core english language library\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5aee2",
   "metadata": {},
   "source": [
    "Below 'u' stands for unicode string. Here each of the words will be treated as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fa54a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a document object\n",
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe048f",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "token.text => returns text of individual token\n",
    "\n",
    "token.pos => returns codes of parts of speech for individual tokens\n",
    "\n",
    "token.pos_ => returns names of parts of speech for individual tokens\n",
    "\n",
    "token.dep_ => returns syntactic dependency of individual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5abeb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla 95 PROPN nsubj\n",
      "is 99 VERB aux\n",
      "looking 99 VERB ROOT\n",
      "at 84 ADP prep\n",
      "buying 99 VERB pcomp\n",
      "U.S. 95 PROPN compound\n",
      "startup 91 NOUN dobj\n",
      "for 84 ADP prep\n",
      "$ 98 SYM quantmod\n",
      "6 92 NUM compound\n",
      "million 92 NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# Every parts of speech has a code in NLP\n",
    "for token in doc:\n",
    "    print(token.text, token.pos, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c37df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x7f5fe74951d0>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x7f5fe6e18590>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x7f5fe6e18b30>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652eba40",
   "metadata": {},
   "source": [
    "Above our text is entering processing pipeline. Here the text broken down and then enters a series of operation i.e. tagging, parsing and describing the data. \n",
    "\n",
    "Basic NLP pipeline has a tagger, parser and a 'ner'(name-entity recogniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99afabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns list of pipeline operations\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f1b49",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "\n",
    "The very first step in processing any text is to split up all the component parts i.e. words and punctuation into tokens and these tokens are annotated inside doc object to contain descriptive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148da4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c4de672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    }
   ],
   "source": [
    "# Here we take a span of the entire document\n",
    "life_quote = doc2[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8e38444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6528d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d20b963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'This is the first sentence. This is the second sentence. This is the last sentence.')\n",
    "for sentence in doc3.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e32f3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True None\n"
     ]
    }
   ],
   "source": [
    "# returns TRUE if it is the start of the sentence\n",
    "# returns NONE if it is not the start of the sentence\n",
    "print(doc3[6].is_sent_start, doc3[7].is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ffc75",
   "metadata": {},
   "source": [
    "TOKENIZATION - part-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8b3b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4092fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "998d14bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 96 PUNCT punct\n",
      "We 94 PRON nsubj\n",
      "'re 99 VERB aux\n",
      "moving 99 VERB ROOT\n",
      "to 84 ADP prep\n",
      "L.A. 95 PROPN pobj\n",
      "! 96 PUNCT punct\n",
      "\" 96 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "str1 = '\"We\\'re moving to L.A.!\"'\n",
    "doc1 = nlp(str1)\n",
    "for t in doc1:\n",
    "    print(t.text, t.pos, t.pos_, t.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2eda9b",
   "metadata": {},
   "source": [
    "-  **Prefix**:\tCharacter(s) at the beginning &#9656; `$ ( “ ¿`\n",
    "-  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ”`\n",
    "-  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "-  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0a276",
   "metadata": {},
   "source": [
    "Notice that tokens are pieces of the original text. That is, we don't see any conversion to word stems or lemmas (base forms of words) and we haven't seen anything about organizations/places/money etc. Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576af233",
   "metadata": {},
   "source": [
    "## Prefixes, Suffixes and Infixes\n",
    "spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b306999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We 94 PRON nsubj\n",
      "'re 99 VERB ROOT\n",
      "here 85 ADV advmod\n",
      "to 93 PART aux\n",
      "help 99 VERB advcl\n",
      "! 96 PUNCT punct\n",
      "Send 99 VERB ROOT\n",
      "snail 91 NOUN compound\n",
      "- 96 PUNCT punct\n",
      "mail 91 NOUN dobj\n",
      ", 96 PUNCT punct\n",
      "email 91 NOUN conj\n",
      "support@oursite.com 100 X ROOT\n",
      "or 88 CCONJ cc\n",
      "visit 99 VERB conj\n",
      "us 94 PRON dobj\n",
      "at 84 ADP prep\n",
      "http://www.oursite.com 100 X pobj\n",
      "! 96 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "for t in doc2:\n",
    "    print(t.text, t.pos, t.pos_, t.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb93a3",
   "metadata": {},
   "source": [
    "<font color=green>Note that the exclamation points, comma, and the hyphen in 'snail-mail' are assigned their own tokens, yet both the email address and website are preserved.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb4cd860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 89 DET det\n",
      "5 92 NUM nummod\n",
      "km 91 NOUN compound\n",
      "NYC 95 PROPN compound\n",
      "cab 91 NOUN compound\n",
      "ride 91 NOUN nsubj\n",
      "costs 99 VERB ROOT\n",
      "$ 98 SYM nmod\n",
      "10.30 92 NUM dobj\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
    "for t in doc3:\n",
    "    print(t.text, t.pos, t.pos_, t.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d210c5",
   "metadata": {},
   "source": [
    "<font color=green>Here the distance unit and dollar sign are assigned their own tokens, yet the dollar amount is preserved.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c304b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let 99 VERB ROOT\n",
      "'s 94 PRON nsubj\n",
      "visit 99 VERB ccomp\n",
      "St. 95 PROPN compound\n",
      "Louis 95 PROPN dobj\n",
      "in 84 ADP prep\n",
      "the 89 DET det\n",
      "U.S. 95 PROPN pobj\n",
      "next 83 ADJ amod\n",
      "year 91 NOUN npadvmod\n",
      ". 96 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
    "for t in doc4:\n",
    "    print(t.text, t.pos, t.pos_, t.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ca62f",
   "metadata": {},
   "source": [
    "<font color=green>Here the abbreviations for \"Saint\" and \"United States\" are both preserved.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893170f7",
   "metadata": {},
   "source": [
    "## COUNTING TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acb09bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns list of tokens\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2d76bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x7f5fd3f2bf80>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the total vocabulary in the 'en_core_web_sm' language library\n",
    "doc.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d1cd2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57852"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns total number of vocabulary in the 'en_core_web_sm' language library\n",
    "len(doc.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91a79fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d524413e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76996/3402380947.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reassignment is not not allowed for document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hello'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# reassignment is not not allowed for document\n",
    "doc[0] = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c4b0a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "ORG\n",
      "Companies, agencies, institutions, etc.\n",
      "\n",
      "\n",
      "India\n",
      "GPE\n",
      "Countries, cities, states\n",
      "\n",
      "\n",
      "$6 million\n",
      "MONEY\n",
      "Monetary values, including unit\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'Apple to build a India factory for $6 million')\n",
    "for entity in doc2.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_)\n",
    "    print(spacy.explain(entity.label_))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495238d",
   "metadata": {},
   "source": [
    "LABELS:\n",
    "    \n",
    "ORG: organization\n",
    "GPE: Geo political Entity\n",
    "MONEY: Monetary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e480dc4",
   "metadata": {},
   "source": [
    "## NOUNS CHUNKS\n",
    "Similar to Doc.ents, Doc.noun_chunks are another object property. Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in Sheb Wooley's 1958 song, a \"one-eyed, one-horned, flying, purple people-eater\" would be one long noun chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3217fc",
   "metadata": {},
   "source": [
    "Noun chunks can be thought of a nouns + the words describing that noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdd1ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Autonomous cars shift insurance liability towards manufacturers.')\n",
    "for nchunks in doc.noun_chunks:\n",
    "    print(nchunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc0619",
   "metadata": {},
   "source": [
    "We'll look at additional noun_chunks components besides `.text` in an upcoming section.<br>For more info on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f83901",
   "metadata": {},
   "source": [
    "## VISUALIZER\n",
    "\n",
    "___\n",
    "# Built-in Visualizers\n",
    "\n",
    "spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When you export your notebook, the visualizations will be included as HTML.\n",
    "\n",
    "For more info visit https://spacy.io/usage/visualizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5125da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5f996bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp  = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba71e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Apple is going to build a U.K. factory for $6 million.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14bce546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1010\" height=\"297.0\" style=\"max-width: none; height: 297.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,162.0 C70,82.0 200.0,82.0 200.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,164.0 L62,152.0 78,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M150,162.0 C150,122.0 195.0,122.0 195.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,164.0 L142,152.0 158,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M310,162.0 C310,122.0 355.0,122.0 355.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,164.0 L302,152.0 318,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M230,162.0 C230,82.0 360.0,82.0 360.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M360.0,164.0 L368.0,152.0 352.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M470,162.0 C470,82.0 600.0,82.0 600.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,164.0 L462,152.0 478,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M550,162.0 C550,122.0 595.0,122.0 595.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,164.0 L542,152.0 558,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M390,162.0 C390,42.0 605.0,42.0 605.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M605.0,164.0 L613.0,152.0 597.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M390,162.0 C390,2.0 690.0,2.0 690.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M690.0,164.0 L698.0,152.0 682.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M790,162.0 C790,82.0 920.0,82.0 920.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,164.0 L782,152.0 798,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M870,162.0 C870,122.0 915.0,122.0 915.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,164.0 L862,152.0 878,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M710,162.0 C710,42.0 925.0,42.0 925.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,164.0 L933.0,152.0 917.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# style = 'dep' is syntactic dependency\n",
    "displacy.render(doc,style='dep',jupyter=True,options={'distance':80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0fa47b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b6436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (825 kB)\n",
      "\u001b[K     |████████████████████████████████| 825 kB 32.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[K     |████████████████████████████████| 490 kB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 20.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: jinja2 in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /home/santosh/Anaconda/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/santosh/Anaconda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/santosh/Anaconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/santosh/Anaconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/santosh/Anaconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/santosh/Anaconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/santosh/Anaconda/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/santosh/Anaconda/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.4 smart-open-6.3.0 spacy-3.5.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.7 typer-0.7.0 typing-extensions-4.4.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5365402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "94\n",
      "PRON\n",
      "pronoun\n",
      "\n",
      "\n",
      "is\n",
      "99\n",
      "VERB\n",
      "verb\n",
      "\n",
      "\n",
      "a\n",
      "89\n",
      "DET\n",
      "determiner\n",
      "\n",
      "\n",
      "nice\n",
      "83\n",
      "ADJ\n",
      "adjective\n",
      "\n",
      "\n",
      "person\n",
      "91\n",
      "NOUN\n",
      "noun\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'he is a nice person')\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print(token.pos)\n",
    "    print(token.pos_)\n",
    "    print(spacy.explain(token.pos_))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d2793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41cbb85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages (3.3)\r\n",
      "Requirement already satisfied: six in /home/santosh/Anaconda/envs/nlp_course/lib/python3.7/site-packages (from nltk) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b6691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebe0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24ef063",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner', 'ran', 'runs', 'easily','fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc7fb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run------->run\n",
      "runner------->runner\n",
      "ran------->ran\n",
      "runs------->run\n",
      "easily------->easili\n",
      "fairly------->fairli\n",
      "fairness------->fair\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer is not so sophisticated stemmer, sometimes gives some interesting weird stems like for words\n",
    "# easily and fairly\n",
    "# Here Porter stemmer for easily able to identify runner as a noun hence it is not stemmed \n",
    "for word in words:\n",
    "    print(word + '------->' + ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "077f8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f0ad44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a46451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run------->run\n",
      "runner------->runner\n",
      "ran------->ran\n",
      "runs------->run\n",
      "easily------->easili\n",
      "fairly------->fair\n",
      "fairness------->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + '------->' + sbs.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02445a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = ['generation', 'generate', 'generous', 'generously']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b22c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation------>generat\n",
      "generate------>generat\n",
      "generous------>generous\n",
      "generously------>generous\n"
     ]
    }
   ],
   "source": [
    "for word in words2:\n",
    "    print(word + '------>' + sbs.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05697dbe",
   "metadata": {},
   "source": [
    "## LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41fa0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for t in text:\n",
    "        print(f'{t.text:{15}} {t.pos_:{5}} {t.lemma:<{30}} {t.lemma_:{20}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8dd7c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I               PRON  561228191312463089             -PRON-              \n",
      "am              VERB  10382539506755952630           be                  \n",
      "runner          ADV   12640964157389618806           runner              \n",
      "running         VERB  12767647472892411841           run                 \n",
      "in              ADP   3002984154512732771            in                  \n",
      "a               DET   11901859001352538922           a                   \n",
      "race            NOUN  8048469955494714898            race                \n",
      "because         ADP   16950148841647037698           because             \n",
      "I               PRON  561228191312463089             -PRON-              \n",
      "love            VERB  3702023516439754181            love                \n",
      "to              PART  3791531372978436496            to                  \n",
      "run             VERB  12767647472892411841           run                 \n",
      "since           ADP   10066841407251338481           since               \n",
      "I               PRON  561228191312463089             -PRON-              \n",
      "ran             VERB  12767647472892411841           run                 \n",
      "today           NOUN  11042482332948150395           today               \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am runner running in a race because I love to run since I ran today')\n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693121e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I               PRON  561228191312463089             -PRON-              \n",
      "saw             VERB  11925638236994514241           see                 \n",
      "ten             NUM   7970704286052693043            ten                 \n",
      "mice            NOUN  1384165645700560590            mouse               \n",
      "today           NOUN  11042482332948150395           today               \n",
      "!               PUNCT 17494803046312582752           !                   \n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'I saw ten mice today!')\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03e389",
   "metadata": {},
   "source": [
    "## STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eceec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', 'being', 'onto', 'top', 'ever', 'had', 'serious', 'until', 'back', 'here', 'before', 'please', 'unless', 'six', 'enough', 'everyone', 'side', 'toward', 'her', 'something', 'used', 'which', 'becomes', 'my', 'your', 'once', 'so', 'this', 'very', 'have', 'some', 'me', 'though', 'three', 'hereafter', 'sometime', 'such', 'via', 'whereby', 'afterwards', 'beforehand', 'any', 'nevertheless', 'no', 'moreover', 'somehow', 'seemed', 'hundred', 'may', 'would', 'go', 'own', 'others', 'sometimes', 'ten', 'two', 'itself', 'too', 'yourself', 'make', 'indeed', 'third', 'themselves', 'whereupon', 'must', 'therefore', 'do', 'is', 'could', 'less', 'call', 'during', 'first', 'himself', 'various', 'whom', 'further', 'ca', 'herself', 'show', 'behind', 'these', 'became', 'she', 'thru', 'perhaps', 'does', 'on', 'namely', 'even', 'see', 'off', 'can', 'among', 'someone', 'twenty', 'thereafter', 'well', 'those', 'towards', 'whose', 'why', 'beside', 'quite', 'then', 'one', 'name', 'without', 'cannot', 'as', 'all', 'front', 'been', 'take', 'an', 'rather', 'our', 'to', 'each', 'am', 'i', 'above', 'he', 'yours', 'done', 'again', 'from', 'than', 'them', 'there', 'next', 'through', 'herein', 'whether', 'across', 'everything', 'none', 'same', 'between', 'wherever', 'whole', 'yet', 'how', 'thus', 'fifteen', 'becoming', 'the', 'eleven', 'since', 'while', 'seems', 'be', 'hence', 'under', 'about', 'however', 'nobody', 'twelve', 'last', 'that', 'due', 'you', 'we', 'along', 'empty', 'really', 'around', 'what', 'a', 'might', 'whatever', 'four', 'another', 'its', 'more', 'seeming', 'else', 'become', 'not', 'was', 'fifty', 'in', 'many', 'few', 'myself', 'neither', 'other', 'just', 'within', 'him', 'against', 'should', 'much', 'sixty', 'mostly', 'anyway', 'they', 'also', 'hers', 'put', 'bottom', 'never', 'nothing', 'move', 'still', 'are', 'because', 'keep', 'wherein', 'formerly', 'amount', 'seem', 'five', 'former', 'whereafter', 'thereupon', 'over', 'out', 'were', 'anything', 'after', 'only', 'if', 'always', 'has', 'somewhere', 'by', 'give', 'almost', 'into', 'full', 'meanwhile', 'nor', 'or', 'up', 'upon', 'nowhere', 'whenever', 'will', 'of', 'for', 'whither', 'did', 'therein', 'throughout', 'beyond', 'whereas', 'besides', 'but', 'where', 'thence', 'per', 'both', 'together', 'using', 'down', 'whoever', 'yourselves', 'although', 'and', 'get', 'anyone', 'doing', 'made', 'mine', 'ourselves', 're', 'it', 'noone', 'anyhow', 'below', 'thereby', 'most', 'alone', 'regarding', 'often', 'now', 'their', 'every', 'except', 'eight', 'forty', 'part', 'latter', 'several', 'amongst', 'already', 'either', 'ours', 'when', 'whence', 'hereby', 'latterly', 'who', 'with', 'least', 'everywhere', 'us', 'at', 'nine', 'otherwise', 'hereupon', 'say', 'anywhere', 'elsewhere'}\n"
     ]
    }
   ],
   "source": [
    "# returns a set of all the stop words in the dictionary\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3c0c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cda02c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lexeme.Lexeme at 0x7f352007a7d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8014d021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['an'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c68349ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86746548",
   "metadata": {},
   "source": [
    "Adding stop words to default set of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "545d4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "162c43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "240c53c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f0a969f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539d64a",
   "metadata": {},
   "source": [
    "Removing a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84c4a82c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'btw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148619/3871519440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'btw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'btw'"
     ]
    }
   ],
   "source": [
    "nlp.Defaults.stop_words.remove('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34ede38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "893c8a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58977b72",
   "metadata": {},
   "source": [
    "## Phrase Matching and Vocabulary\n",
    "\n",
    "Here we will identify and label specific phrases that match patterns we can define ourselves.\n",
    "\n",
    "### Rule Based Matching \n",
    "\n",
    "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "734d6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b9d1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matcher library\n",
    "from spacy.matcher import Matcher\n",
    "mtchr = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3be0f",
   "metadata": {},
   "source": [
    "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c5635c",
   "metadata": {},
   "source": [
    "#### Creating Patterns\n",
    "\n",
    "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7babb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrn1 = [{'LOWER': 'solarpower'}]\n",
    "ptrn2 = [{'LOWER': 'solar'},{'LOWER':'power'}]\n",
    "ptrn3 = [{'LOWER': 'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "\n",
    "mtchr.add('SolarPower', None, ptrn1, ptrn2, ptrn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcdefd",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
    "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
    "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
    "\n",
    "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
    "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d10e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand for solarpower increases.Solar-power cars are gaining popularity.')\n",
    "found_matches = mtchr(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fc2af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "\n",
      "\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "\n",
      "\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    str_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, str_id, start, end, span.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aea136",
   "metadata": {},
   "source": [
    "The `match_id` is simply the hash value of the `str_id` 'SolarPower'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3453e0c",
   "metadata": {},
   "source": [
    "### Setting pattern options and quantifiers\n",
    "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2088a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrn1 = [{'LOWER': 'solarpower'}]\n",
    "ptrn2 = [{'LOWER':'solar'},{'IS_PUNCT':True, 'OP':'*'},{'LOWER':'power'}]\n",
    "\n",
    "# removing old matcher with stringid SolarPower\n",
    "mtchr.remove('SolarPower')\n",
    "\n",
    "# adding the new matcher \n",
    "mtchr.add('Solar_Power', None, ptrn1, ptrn2)\n",
    "\n",
    "match_found = mtchr(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "464c54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9627793059523059485 Solar_Power 1 3 Solar Power\n",
      "\n",
      "\n",
      "9627793059523059485 Solar_Power 10 11 solarpower\n",
      "\n",
      "\n",
      "9627793059523059485 Solar_Power 13 16 Solar-power\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mtch_id, start, end in match_found:\n",
    "    str_id = nlp.vocab.strings[mtch_id]\n",
    "    span = doc[start:end]\n",
    "    print(mtch_id, str_id, start, end, span.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31742a",
   "metadata": {},
   "source": [
    "This found both two-word patterns, with and without the hyphen!\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ba3ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice -start\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69f61433",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b529275",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtchr = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b2e1b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrn1 = [{'LOWER': 'solarpower'}]\n",
    "ptrn2 = [{'LOWER': 'solar'},{'IS_PUNCT': True, 'OP': '*'},{'LOWER': 'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ae378a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtchr.add('SolarPower', None, ptrn1, ptrn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a244912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Solarpower is the Solar--power of the Solar power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "916bdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 1), (8656102463236116519, 3, 6), (8656102463236116519, 8, 10)]\n"
     ]
    }
   ],
   "source": [
    "mtchs_found = mtchr(doc)\n",
    "print(mtchs_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6507f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolarPower 0 1 Solarpower\n",
      "SolarPower 3 6 Solar--power\n",
      "SolarPower 8 10 Solar power\n"
     ]
    }
   ],
   "source": [
    "for mtch_id, start, end in mtchs_found:\n",
    "    match_str = nlp.vocab.strings[mtch_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_str, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2359b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtchr.remove('SolarPower')\n",
    "#pratice-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9b8ac",
   "metadata": {},
   "source": [
    "### Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af54020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d02f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "488d7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrs_mtchr = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cd16f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/santosh/Desktop/MASTERS/EXTRA_COURSE_WORK/NLP/UPDATED_NLP_COURSE/TextFiles/reaganomics.txt', encoding='utf8', errors='ignore') as myfile:\n",
    "    doc = nlp(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e4bf6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a list of match phrases:\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "\n",
    "phrs_ptrn = [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "661300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrs_mtchr.add('economics', None, *phrs_ptrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80501fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2666819224875146687, 41, 45), (2666819224875146687, 49, 53), (2666819224875146687, 54, 56), (2666819224875146687, 61, 65), (2666819224875146687, 673, 677), (2666819224875146687, 2984, 2988)]\n"
     ]
    }
   ],
   "source": [
    "found_mtchs = phrs_mtchr(doc)\n",
    "print(found_mtchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a733a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2666819224875146687 economics 41 45 supply-side economics\n",
      "2666819224875146687 economics 49 53 trickle-down economics\n",
      "2666819224875146687 economics 54 56 voodoo economics\n",
      "2666819224875146687 economics 61 65 free-market economics\n",
      "2666819224875146687 economics 673 677 supply-side economics\n",
      "2666819224875146687 economics 2984 2988 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for mtch_id, start, end in found_mtchs:\n",
    "    mtch_str = nlp.vocab.strings[mtch_id]\n",
    "    span = doc[start:end]\n",
    "    print(mtch_id, mtch_str, start, end, span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
